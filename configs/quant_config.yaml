dataset:
  scale: 4
  search_subsample: 0.7 # Search is too slow. search is too slow. Therefore we use only part of the data.
  batch_size: 32
  crop_size: 45 # small image,  40x4=160, pathces are 200x200
  files_list:  /home/dev/data_main/SR/DIV2K/processed_200_st70/HR
  debug_mode: False # run on a small porion of data
  
arch:
  bits: [8,4] #bitwidth options
  channels: 3 # input channels
  c_fixed: 36 # channels 
  scale: 4
  arch_pattern: # the number of blocks of different types
      head: 2
      skip: 1
      tail: 2
      body: 2
      upsample: 1
  body_cells: 3 # body consists of several repetitions of identical blocks
  skip_mode: False # If False -> use ADM
  primitives:
    head: ["simple_3x3", "simple_5x5", "simple_3x3_grouped_3", "simple_5x5_grouped_3"]
    body: ["conv_5x1_1x5", "conv_3x1_1x3", "simple_3x3", "simple_5x5"]
    skip: ["simple_1x1", "simple_3x3", "simple_5x5"]
    tail: ["simple_1x1", "simple_3x3", "simple_5x5","simple_5x5_grouped_3","simple_3x3_grouped_3"]
    upsample: ["simple_5x5_grouped_3",simple_3x3_grouped_3, "simple_3x3", "simple_5x5"]

env:
  run_name: 'Recovery_final_model' # name of the experiment. Ignored when 'batch_exp.py' is used.
  gpu: 0 # Device. Ignored when 'batch_exp.py' is used.
  log_dir: /home/dev/data_main/LOGS/QUANT/ # Directory for experiments output.
  im_dir: 'arch_images' # Subfolder of the experiment in which images of architectures are saved.
  workers: 4 
  seed : 777 
  print_freq: 60 # print frequency
 
search:
  # From what epoch to start alpha updates
  warm_up: 0
  # Path to pretrained SUPERNET.
  load_path: #/home/dev/data_main/LOGS/QUANT/SEARCH_PRETRAIN_V2-2021-12-10-17/best.pth.tar
  
  penalty: 0.0 # Flops penalty. Ignored when 'batch_exp.py' is used.
  alpha_selector: softmax # Alpha transformation. 'softmax' recommended. 
  sparse_type: entropy # Sparsity penalty. Available options: 'l1_softmax', 'l1', 'entropy', 'none'.
  sparse_coef: 1e-3 #1e-4 # Coefficient. Recommend: 0.001 for entropy, 0.0001 for l1.
  optimizer: adam # Weights optimizer. Options: 'sgd', 'adam'
  quant_noise: True

  lr_scheduler: cosine # Lr scheduler for weights. 
  w_lr: 1e-3 # lr for weights
  w_momentum: 0.9 # Momentum for weights('sgd' only)
  w_weight_decay: 3e-8 # Weight decay for weights
  w_grad_clip: 5 # gradient clipping for weights
  epochs: 20 # number of search epochs
  alpha_lr:  3e-4 # lr for alpha
  alpha_weight_decay: 0 #1e-3 #weight decay for alpha
  temp_max: 10 # Only valid for alpha_selector=='gumbel'. 
  temp_min: 0.1 # Same as above
  
train:
  warm_up: 0 # Number of pretrain epochs in full precision.
  lr_scheduler: cosine # Lr scheduler. 
  lr : 1e-4 # Lr for weights
  weight_decay : 0 # Weight decay
  print_freq : 200 # print frequency
  epochs : 30 # Number of training epochs
  # Genotype of selected model. Ignored when 'batch_exp.py' is used. 
  genotype_path:  /home/dev/data_main/LOGS/QUANT/recover/trail_1/SEARCH_batch_experiment_penalty_0_trail_1-2022-02-05-20/best_arch.gen


